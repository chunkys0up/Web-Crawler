# Web-Crawler

Uses breadth first search to parse through English Wikipedia articles.

In each Goroutine, make sure that the we can crawl through the URL and fetch its html code. Then parse through it to find all the Wikipedia articles. Next it loops through the articles to make sure that they are distinct and add them to the queue. 
